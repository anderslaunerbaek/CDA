{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_class import my_class as my\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(pred, Y):\n",
    "    \"\"\"\n",
    "    asd\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    def array_to_latex(tbl):\n",
    "        for ii in range(tbl.shape[0]):\n",
    "            tmp_str = ''\n",
    "            for jj in range(tbl.shape[1]):\n",
    "                if jj != 0:\n",
    "                    tmp_str += ' & ' + \"{:.0f}\".format(tbl[ii,jj])  \n",
    "                else:\n",
    "                    tmp_str += \"{:.0f}\".format(tbl[ii,jj]) \n",
    "\n",
    "            tmp_str += ' \\\\\\\\ '\n",
    "            print(tmp_str)\n",
    "\n",
    "    def performance_measure(pred_test, Y_test):\n",
    "        #\n",
    "        cm = confusion_matrix(y_pred = pred_test,\n",
    "            y_true = Y_test, \n",
    "            labels = list(range(len(set(Y_test)))))\n",
    "        TP = np.diag(cm)\n",
    "        FP = np.sum(cm, axis=0) - np.diag(cm)\n",
    "        FN = np.sum(cm,axis=1) - np.diag(cm)\n",
    "        TN = np.sum(cm) - (FP+FN+TP)\n",
    "        #\n",
    "        precision = TP/ (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1 = np.multiply(2, np.multiply(precision, recall) / np.add(precision, recall))\n",
    "        acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "        #\n",
    "        return TP, FP, precision, recall, F1, acc, cm\n",
    "\n",
    "\n",
    "    TP, FP, precision, recall, F1, Acc, cm = performance_measure(pred_test=pred, Y_test=np.argmax(Y, axis=1))\n",
    "    print('--------------------------------------------')\n",
    "    print('Average for all classes')\n",
    "    print('Accurcy:   %f' %(np.mean(Acc)))\n",
    "    print('Precision: %f' %(np.mean(precision)))\n",
    "    print('Recall:    %f' %(np.mean(recall)))\n",
    "    print('F1:        %f' %(np.mean(F1)))\n",
    "\n",
    "    #\n",
    "    print(\"std.\\n\")\n",
    "    array_to_latex(cm)\n",
    "    # \n",
    "    print(\"\\npct.\\n\")\n",
    "    cm_norm = cm / cm.astype(np.float).sum(axis=1, keepdims=True) * 100\n",
    "    array_to_latex(cm_norm)\n",
    "\n",
    "    print(\"\\n\\nPaste into latex..\\n\\n\")\n",
    "    tmp = np.ndarray((2,6))\n",
    "    tmp[0:2,0:2] = cm_norm\n",
    "    \n",
    "    tmp[0:2,2] = precision * 100\n",
    "    tmp[0:2,3] = recall * 100\n",
    "    tmp[0:2,4] = F1 * 100\n",
    "    tmp[0:2,5] = Acc * 100\n",
    "    #\n",
    "    array_to_latex(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set directories\n",
    "data_path_clear = \"./../data/data/clear/skive/2016/\"\n",
    "data_path_foggy = \"./../data/data/foggy/skive/2016/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pic_path_clear = my.list_pics(data_path_clear)\n",
    "pic_path_foggy = my.list_pics(data_path_foggy)\n",
    "pic_path = pic_path_clear + pic_path_foggy\n",
    "pic_path = [pic_path[ii] for ii in range(len(pic_path)) if \".jpg\" in pic_path[ii]]\n",
    "\n",
    "n_clear = len(pic_path_clear)\n",
    "n_foggy = len(pic_path_foggy)\n",
    "n = len(pic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create target variable and feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_clear = np.zeros(n_clear, dtype=int)\n",
    "Y_foggy = np.ones(n_foggy, dtype=int)\n",
    "Y = np.concatenate((Y_clear, Y_foggy), axis=0)\n",
    "# balance(Y)\n",
    "# one hot\n",
    "b = np.zeros((len(Y), len(set(Y))))\n",
    "b[np.arange(len(Y)), Y] = 1\n",
    "Y = b\n",
    "n_classes = Y.shape[1]\n",
    "classes = [\"clear\", \"foggy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images to array!\n"
     ]
    }
   ],
   "source": [
    "ratio = 1\n",
    "channels = 3\n",
    "pics = my.img_to_nparr(pic_path=pic_path, \n",
    "                       img_height = 576, \n",
    "                       img_width = 704, \n",
    "                       rat = ratio,\n",
    "                       ch = channels,\n",
    "                       verbose = False)\n",
    "# only consider the 3 /5 top of the picture...\n",
    "# pics = pics[:, 0:int(pics.shape[1] / 5 * 4),:,:]\n",
    "# dimensions picture\n",
    "image_height, image_width, _ = pics[1].shape\n",
    "\n",
    "n_pixels = image_height * image_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\"Dark channel\", \"sobel_VARsob\", \"sobel_TEN\", \n",
    "            \"laplace_sum\", \"laplace_var\", \"pct_overexposed\"]\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update = False\n",
    "#\n",
    "if update:\n",
    "    X = np.zeros((n, n_features))\n",
    "    for ii in range(n):\n",
    "        print(str(ii + 1) + \" of \" + str(n), end=\"\\r\")\n",
    "        feature_list = []\n",
    "        # dark channel\n",
    "        dc = my.get_dark_channel(pics[ii], win=20)\n",
    "        # close to 1 -> presents of fog\n",
    "        feature_list.append(np.mean(dc / 255.0)) \n",
    "\n",
    "        # sobel edge filtering\n",
    "        S = my.sobel_filter(pics[ii]),\n",
    "        feature_list.append(my.VARsob(S))\n",
    "        feature_list.append(my.TEN(S) / n_pixels)\n",
    "        \n",
    "        # laplace\n",
    "        L = my.lapalce_filter(pics[ii])\n",
    "        feature_list.append(np.sum(abs(L)) / n_pixels)\n",
    "        feature_list.append(np.var(abs(L)) / n_pixels)\n",
    "        \n",
    "        # pct. overexposed pixels\n",
    "        feature_list.append(my.overexposed_pixels(pics[ii]) / n_pixels)\n",
    "        \n",
    "        # add to design matrix\n",
    "        X[ii,:] = feature_list\n",
    "    # if updated save new... \n",
    "    print(\"Updated...\")\n",
    "    np.save(\"./../data/tmp/X.npy\", X)\n",
    "else:\n",
    "    X = np.load(\"./../data/tmp/X.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomize the order of the pictures\n",
    "idx = np.arange(n)\n",
    "np.random.shuffle(idx)\n",
    "#\n",
    "pic_path = np.array(pic_path)[idx]\n",
    "Y = Y[idx]\n",
    "X = X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and val. size:\t190\n",
      "Test set size:\t\t82\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "test_size = 0.3\n",
    "rand_state = 22\n",
    "K = 2\n",
    "#Splitting \n",
    "#X_model, X_test, Y_model, Y_test = train_test_split(X, Y,\n",
    "#                                                    test_size = test_size,\n",
    "#                                                    random_state = rand_state)\n",
    "idx_X_model, idx_X_test, idx_Y_model, idx_Y_test = train_test_split(np.arange(n),np.arange(n),\n",
    "                                                                    test_size = test_size,\n",
    "                                                                    random_state = rand_state)\n",
    "# devide data\n",
    "X_model, X_test, Y_model, Y_test = X[idx_X_model], X[idx_X_test], Y[idx_Y_model], Y[idx_Y_test]\n",
    "pic_path_model, pic_path_test = pic_path[idx_X_model], pic_path[idx_X_test]\n",
    "\n",
    "np.save(\"./../data/tmp/X_model.npy\", X_model)\n",
    "\n",
    "\n",
    "print(\"Train and val. size:\\t{0}\\nTest set size:\\t\\t{1}\".format(len(X_model), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "clf = RandomForestClassifier(random_state = rand_state,\n",
    "                             n_jobs = -1,\n",
    "                             class_weight = {0: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[0], \n",
    "                                             1: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[1]})\n",
    "# clf.get_params()\n",
    "# RF serach grid\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = clf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 5, verbose=1, \n",
    "                               random_state=rand_state, \n",
    "                               n_jobs = -1)\n",
    "update = False\n",
    "#\n",
    "if update:\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X=X_model, y=np.argmax(Y_model, 1))\n",
    "    print(rf_random.best_params_)\n",
    "else:\n",
    "    print({'bootstrap': True,\n",
    "     'max_depth': 20,\n",
    "     'max_features': 'auto',\n",
    "     'min_samples_leaf': 1,\n",
    "     'min_samples_split': 5,\n",
    "     'n_estimators': 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./../data/tmp/clf.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'min_samples_leaf': [1,2],\n",
    "    'min_samples_split': [2,3],\n",
    "    'n_estimators': [150,200,250]}\n",
    "# Instantiate the grid search model\n",
    "\n",
    "clf = RandomForestClassifier(random_state = rand_state,\n",
    "                             n_jobs = -1,\n",
    "                             bootstrap = True,\n",
    "                             max_features = 'auto',\n",
    "                             class_weight = {0: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[0], \n",
    "                                             1: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[1]})\n",
    "\n",
    "grid_search = GridSearchCV(estimator = clf, \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 5, n_jobs = -1, verbose = 1)\n",
    "#\n",
    "if update:\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_model, np.argmax(Y_model, 1))\n",
    "    print(grid_search.best_params_)\n",
    "else:\n",
    "    print({'max_depth': 3, \n",
    "           'min_samples_leaf': 1, \n",
    "           'min_samples_split': 2, \n",
    "           'n_estimators': 200})\n",
    "    \n",
    "#\n",
    "\n",
    "clf = RandomForestClassifier(max_depth = 5,\n",
    "                             min_samples_leaf = 1,\n",
    "                             min_samples_split = 3,\n",
    "                             n_estimators = 300,\n",
    "                             random_state = rand_state,\n",
    "                             n_jobs = -1,\n",
    "                             bootstrap = True,\n",
    "                             max_features = 'auto',\n",
    "                             class_weight = {0: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[0], \n",
    "                                             1: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[1]})\n",
    "\n",
    "# fit tree\n",
    "clf.fit(X_model, np.argmax(Y_model, 1))\n",
    "joblib.dump(clf, \"./../data/tmp/clf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'brute', 'leaf_size': 2, 'n_neighbors': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./../data/tmp/neigh.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [int(x) for x in np.linspace(start = 2, stop = 40, num = 1)],\n",
    "    'n_neighbors': [int(x) for x in np.linspace(start = 2, stop = 50, num = 1)]}\n",
    "# Instantiate the grid search model\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = neigh, \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 5, n_jobs = -1, verbose = 1)\n",
    "#\n",
    "\n",
    "#\n",
    "if update:\n",
    "    # Fit the random search model\n",
    "    grid_search.fit(X_model, np.argmax(Y_model, 1))\n",
    "    print(rf_random.best_params_)\n",
    "else:\n",
    "    print({'algorithm': 'brute', \n",
    "           'leaf_size': 2, \n",
    "           'n_neighbors': 2})\n",
    "\n",
    "    \n",
    "# fit knn\n",
    "neigh = KNeighborsClassifier(algorithm = 'brute', \n",
    "                             leaf_size = 2,\n",
    "                             n_neighbors = 2,\n",
    "                             n_jobs=-1)\n",
    "neigh.fit(X_model, np.argmax(Y_model, 1)) \n",
    "joblib.dump(neigh, \"./../data/tmp/neigh.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "--------------------------------------------\n",
      "Average for all classes\n",
      "Accurcy:   0.987805\n",
      "Precision: 0.987805\n",
      "Recall:    0.988095\n",
      "F1:        0.987803\n",
      "std.\n",
      "\n",
      "41 & 1 \\\\ \n",
      "0 & 40 \\\\ \n",
      "\n",
      "pct.\n",
      "\n",
      "98 & 2 \\\\ \n",
      "0 & 100 \\\\ \n",
      "\n",
      "\n",
      "Paste into latex..\n",
      "\n",
      "\n",
      "98 & 2 & 100 & 98 & 99 & 99 \\\\ \n",
      "0 & 100 & 98 & 100 & 99 & 99 \\\\ \n",
      "KNN\n",
      "--------------------------------------------\n",
      "Average for all classes\n",
      "Accurcy:   0.963415\n",
      "Precision: 0.963415\n",
      "Recall:    0.963690\n",
      "F1:        0.963409\n",
      "std.\n",
      "\n",
      "40 & 2 \\\\ \n",
      "1 & 39 \\\\ \n",
      "\n",
      "pct.\n",
      "\n",
      "95 & 5 \\\\ \n",
      "2 & 98 \\\\ \n",
      "\n",
      "\n",
      "Paste into latex..\n",
      "\n",
      "\n",
      "95 & 5 & 98 & 95 & 96 & 96 \\\\ \n",
      "2 & 98 & 95 & 98 & 96 & 96 \\\\ \n"
     ]
    }
   ],
   "source": [
    "# model assesment\n",
    "clf = joblib.load(\"./../data/tmp/clf.pkl\")\n",
    "print(\"RF\")\n",
    "pred_test = clf.predict(X_test)\n",
    "performance(pred_test, Y_test)\n",
    "\n",
    "neigh = joblib.load(\"./../data/tmp/neigh.pkl\") \n",
    "print(\"KNN\")\n",
    "pred_test = neigh.predict(X_test)\n",
    "performance(pred_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
