{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_class import my_class as my\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set directories\n",
    "data_path_clear = \"./../data/data/clear/skive/2016/\"\n",
    "data_path_foggy = \"./../data/data/foggy/skive/2016/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pic_path_clear = my.list_pics(data_path_clear)\n",
    "pic_path_foggy = my.list_pics(data_path_foggy)\n",
    "pic_path = pic_path_clear + pic_path_foggy\n",
    "pic_path = [pic_path[ii] for ii in range(len(pic_path)) if \".jpg\" in pic_path[ii]]\n",
    "\n",
    "n_clear = len(pic_path_clear)\n",
    "n_foggy = len(pic_path_foggy)\n",
    "n = len(pic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create target variable and feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_clear = np.zeros(n_clear, dtype=int)\n",
    "Y_foggy = np.ones(n_foggy, dtype=int)\n",
    "Y = np.concatenate((Y_clear, Y_foggy), axis=0)\n",
    "# balance(Y)\n",
    "# one hot\n",
    "b = np.zeros((len(Y), len(set(Y))))\n",
    "b[np.arange(len(Y)), Y] = 1\n",
    "Y = b\n",
    "n_classes = Y.shape[1]\n",
    "classes = [\"clear\", \"foggy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images to array!\n"
     ]
    }
   ],
   "source": [
    "ratio = 1\n",
    "channels = 3\n",
    "pics = my.img_to_nparr(pic_path=pic_path, \n",
    "                       img_height = 576, \n",
    "                       img_width = 704, \n",
    "                       rat = ratio,\n",
    "                       ch = channels,\n",
    "                       verbose = False)\n",
    "# only consider the 3 /5 top of the picture...\n",
    "# pics = pics[:, 0:int(pics.shape[1] / 5 * 4),:,:]\n",
    "# dimensions picture\n",
    "image_height, image_width, _ = pics[1].shape\n",
    "\n",
    "n_pixels = image_height * image_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\"Dark channel\", \"sobel_VARsob\", \"sobel_TEN\", \n",
    "            \"laplace_sum\", \"laplace_var\", \"pct_overexposed\"]\n",
    "n_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 of 272\r"
     ]
    }
   ],
   "source": [
    "update = True\n",
    "#\n",
    "if update:\n",
    "    \n",
    "    X = np.zeros((n, n_features))\n",
    "    for ii in range(n):\n",
    "        print(str(ii + 1) + \" of \" + str(n), end=\"\\r\")\n",
    "        feature_list = []\n",
    "        # dark channel\n",
    "        dc = my.get_dark_channel(pics[ii], win=20)\n",
    "        # close to 1 -> presents of fog\n",
    "        feature_list.append(np.mean(dc / 255.0)) \n",
    "\n",
    "        # sobel edge filtering\n",
    "        S = my.sobel_filter(pics[ii]),\n",
    "        feature_list.append(my.VARsob(S))\n",
    "        feature_list.append(my.TEN(S) / n_pixels)\n",
    "        \n",
    "        # laplace\n",
    "        L = my.lapalce_filter(pics[ii])\n",
    "        feature_list.append(np.sum(abs(L)) / n_pixels)\n",
    "        feature_list.append(np.var(abs(L)) / n_pixels)\n",
    "        \n",
    "        # pct. overexposed pixels\n",
    "        feature_list.append(my.overexposed_pixels(pics[ii]) / n_pixels)\n",
    "        \n",
    "        # add to design matrix\n",
    "        X[ii,:] = feature_list\n",
    "    # if updated save new... \n",
    "    print(\"Updated...\")\n",
    "    np.save(\"./../data/tmp/X.npy\", X)\n",
    "else:\n",
    "    X = np.load(\"./../data/tmp/X.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomize the order of the pictures\n",
    "idx = np.arange(n)\n",
    "np.random.shuffle(idx)\n",
    "#\n",
    "pic_path = np.array(pic_path)[idx]\n",
    "Y = Y[idx]\n",
    "X = X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "test_size = 0.3\n",
    "rand_state = 22\n",
    "K = 2\n",
    "#Splitting \n",
    "#X_model, X_test, Y_model, Y_test = train_test_split(X, Y,\n",
    "#                                                    test_size = test_size,\n",
    "#                                                    random_state = rand_state)\n",
    "idx_X_model, idx_X_test, idx_Y_model, idx_Y_test = train_test_split(np.arange(n),np.arange(n),\n",
    "                                                                    test_size = test_size,\n",
    "                                                                    random_state = rand_state)\n",
    "# devide data\n",
    "X_model, X_test, Y_model, Y_test = X[idx_X_model], X[idx_X_test], Y[idx_Y_model], Y[idx_Y_test]\n",
    "pic_path_model, pic_path_test = pic_path[idx_X_model], pic_path[idx_X_test]\n",
    "\n",
    "np.save(\"./../data/tmp/X_model.npy\", X_model)\n",
    "\n",
    "\n",
    "print(\"Train and val. size:\\t{0}\\nTest set size:\\t\\t{1}\".format(len(X_model), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "clf = RandomForestClassifier(n_estimators = 500,\n",
    "                             n_jobs = -1,\n",
    "                             random_state=rand_state,\n",
    "                             max_features = None,\n",
    "                             min_samples_split = 2,\n",
    "                             class_weight = {0: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[0], \n",
    "                                             1: Y_model.shape[0] / (n_classes * np.bincount(np.argmin(Y_model, 1)))[1]},\n",
    "                             max_depth=None)\n",
    "# fit tree\n",
    "clf.fit(X_model, np.argmax(Y_model, 1))\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, \"./../data/tmp/clf.pkl\") \n",
    "\n",
    "# clf = joblib.load(\"./../data/tmp/clf.pkl\")\n",
    "\n",
    "pred_test = clf.predict(X_test)\n",
    "my.performance(pred_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#\n",
    "kk_neigh = [2,3,4,5,6,7,8,9,10]\n",
    "class_error_rate = []\n",
    "for kk in kk_neigh:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=kk,n_jobs=-1)\n",
    "    neigh.fit(X_model, np.argmax(Y_model, 1)) \n",
    "    #\n",
    "    pred_test = neigh.predict(X_test)\n",
    "    class_error_rate.append(my.accuracy(pred_test, Y_test))\n",
    "#\n",
    "neigh = KNeighborsClassifier(n_neighbors=kk_neigh[np.argmax(class_error_rate)],\n",
    "                             n_jobs=-1)\n",
    "neigh.fit(X_model, np.argmax(Y_model, 1)) \n",
    "joblib.dump(neigh, \"./../data/tmp/neigh.pkl\") \n",
    "\n",
    "pred_test = neigh.predict(X_test)\n",
    "my.performance(pred_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50000\n",
    "batch_size = 100\n",
    "display_step = 1000\n",
    "display_step_state = False\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([n_features, n_classes]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "prediction = tf.argmax(pred, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    X_model_bn, X_test_bn = my.batch_normalization(X_model, X_test, epsilon=.0001)\n",
    "    # Training cycle starts\n",
    "    avg_cost = 0\n",
    "    for epoch in range(training_epochs):  \n",
    "        # display logs per epoch step\n",
    "        if ((epoch+1) % display_step == 0) and display_step_state:\n",
    "            print(\"Ep:\", '%04d' % (epoch+1), \n",
    "                  \"\\n\\tcost = {:.5f}\\t{:.5f}\\t{:.5f}\".format(avg_cost/epoch,\n",
    "                                                             sess.run(accuracy, feed_dict={x: X_model_bn, y: Y_model}),\n",
    "                                                             sess.run(accuracy, feed_dict={x: X_test_bn, y: Y_test})))\n",
    "        # \n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_model_bn, y: Y_model})\n",
    "\n",
    "        # aggregate loss\n",
    "        avg_cost += c\n",
    "\n",
    "    # model performance\n",
    "    acc_model = sess.run(accuracy, feed_dict={x: X_model_bn, y: Y_model})\n",
    "    acc_test = sess.run(accuracy, feed_dict={x: X_test_bn, y: Y_test})\n",
    "\n",
    "    # save model for folds\n",
    "    save_path = saver.save(sess, \"./../data/models/model_final.ckpt\")\n",
    "    print(\"Model final saved in path: %s\" % save_path)\n",
    "\n",
    "print(\"\\n\\nFinal model performance: \\n\\tAccuracy model:\\t{:.5f}\\n\\tAccuracy test:\\t{:.5f}\".format(acc_model, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test accuracy\n",
    "with tf.Session() as sess:\n",
    "    # restore variables\n",
    "    saver.restore(sess, \"./../data/models/model_final.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # batch norm\n",
    "    _, X_test_bn = my.batch_normalization(X_model, X_test, epsilon=.0001)\n",
    "    \n",
    "    # Check the values of the variables\n",
    "    acc_test, pred_test = sess.run([accuracy, prediction], feed_dict={x: X_test_bn, y: Y_test})\n",
    "    my.performance(pred_test, Y_test)\n",
    "    \n",
    "    Weights, bias = sess.run([W, b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
