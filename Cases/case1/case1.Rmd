---
title: 'Computational Data Analysis - Case 1'
author: "Anders Launer Bæk (s160159)"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes: 
    - \usepackage{graphicx}
    - \usepackage{hyperref}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo=FALSE, 
                      include=TRUE,
                      warning=FALSE,
                      fig.width=8, fig.height=4,
                      fig.show='hold', fig.align='center',
                      
                      eval=TRUE, 
                      tidy=TRUE, 
                      dev='pdf', 
                      cache=TRUE, fig.pos="th!")

kable_format <- list(small.mark=",",
                     big.mark=',',
                     decimal.mark='.',
                     nsmall=3,
                     digits=3,
                     scientific=FALSE,
                     big.interval=3L)

library(ggplot2)
library(akima)
library(dplyr)
theme_TS <- function(base_size=9, base_family="", face="plain"){
  theme_bw(base_size=base_size, base_family=base_family) %+replace%
    theme(panel.background=element_blank(), 
          panel.border=element_blank(),
          panel.grid=element_blank(),
          axis.text=element_text(size=base_size, face=face, family=base_family),
          axis.title=element_text(size=base_size, face=face, family=base_family),
          legend.text=element_text(size=base_size, face=face, family=base_family))
}

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots=length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol=cols, nrow=ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout=grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind=TRUE))

      print(plots[[i]], vp=viewport(layout.pos.row=matchidx$row,
                                      layout.pos.col=matchidx$col))
    }
  }
}
source("~/DTU/Courses/CDA/Cases/case1/funcs.R")
```

Sparring partner:

* Grétar Atli Grétarsson (s170251)


```{r include=FALSE}
# get data 
dat <- readr::read_csv("~/DTU/Courses/CDA/Cases/case1/Case1_Data.csv") %>% 
    mutate(X100 = as.factor(X100))
# dat[is.na(dat)] <- NA
col_names <- colnames(dat)
```

## Selected models

It has been chosen to estimate a linear model with the below mentioned approaches and combine these individual responses in an final ensemble model.

* Ridge regression: $\beta_{ridge} = \underset { \beta }{arg\, min}\parallel Y-X\beta\parallel_2^2+\lambda \parallel \beta\parallel_2^2$
* Lasso regression: $\beta_{lasso} = \underset { \beta }{arg\, min}\parallel Y-X\beta\parallel_2^2+\lambda \parallel \beta\parallel_1$
* Elastic net: $\beta_{elastic \,net} = \underset { \beta }{arg\, min}\parallel Y-X\beta\parallel_2^2+(1-\alpha) \parallel \beta\parallel_1+\alpha \parallel \beta\parallel_2^2$ where $\alpha =\frac{\lambda_2}{\lambda_2+\lambda_1}=0.5$ in order to have a hybrid bwtween the Rigde and Lasso regression.

The three models use a shrinkage approach in order to force the model coefficients towards zero which reduces the complexity in the model structure.

## Preprocess data
The data provided exists of `r dim(dat)[2]` columns with `r dim(dat)[1]` rows out of which `r dim(dat[!is.na(dat$Y),])[1]` rows includes a valid respond value. 

The following pre-processing have been done to the data set:

* There are `r table(complete.cases(dat[,-1]))[["FALSE"]]` of `r dim(dat)[1]` (`r as.integer(table(complete.cases(dat[,-1]))[["FALSE"]] / dim(dat)[1] * 100)`%) rows with missing data. It has been chosen to impute the missing value for each column by inserting the expected value for the given column. 
* The column X100 is a categorical variable with `r length(levels(dat$X100))` levels (`r paste(levels(dat$X100), collapse = ", ")`). 
It has been chosen to transform X100 in to `r length(levels(dat$X100))` dummy variables: `r paste0("X100",levels(dat$X100), collapse = ", ")`.
* The distribution of each column has been plotted in a histogram in order to investigate the demand for transformations. There is no skewness in the distributions and hereby no need for transformations of the column according to the visual inspections. The levels of the categorical variable (X100) are acceptable balanced overall in train set and in the test set (these without responses), see table \ref{tab_1}.
```{r}
col_means <- dat %>%
    select(-X100) %>%
    summarise_all(funs(mean(., na.rm = T)))
for (ii in col_names[c(-1,-101)]) {
    dat[[ii]][is.na(dat[[ii]])] <- col_means[[ii]]
}

# one hot
dat <- dat %>%
    select(X100) %>% 
    model.matrix(~. -1, data = .) %>% 
    as.data.frame(.) %>% bind_cols(dat, .)
col_names <- colnames(dat)

tmp <- dat %>% select(Y, X100) %>% 
    group_by(X100) %>% 
    summarise(n = as.integer(n()/dim(dat)[1]*100),
              n_train = as.integer(sum(ifelse(!is.na(Y) ,1,0)) / n()*100),
              n_test = as.integer(sum(ifelse(is.na(Y) ,1,0)) / n()*100)) 
tmp$n_train <- as.integer(tmp$n_train / sum(tmp$n_train) * 100)
tmp$n_test <- as.integer(tmp$n_test / sum(tmp$n_test) * 100)
tmp %>% 
    rename(`All (%)` = n, `Train (%)` = n_train, `Test (%)` = n_test) %>%
    knitr::kable(., caption = paste("\\label{tab_1}Percentage balance of the", length(levels(dat$X100)), "levels in the X100 variable."))
```


```{r, eval=FALSE}
for (ii in col_names[c(-1,-101,-102,-103,-104)]) {
    dat %>% select(ii) %>% 
        ggplot(aes(.)) +
        geom_histogram() +
        theme_TS()
    ggsave(paste0("~/DTU/Courses/CDA/Cases/case1/pics/raw/",ii,".pdf"))
}
```


## Prepare data for cross validation train and validate

```{r}
X_mod <- dat %>% filter(!is.na(Y)) %>% select(-Y,-X100)
Y_mod <- dat %>% filter(!is.na(Y)) %>% select(Y)
X_tt <- dat %>% filter(is.na(Y)) %>% select(-Y, -X100) %>% as.matrix(.)

p <- dim(X_mod)[2]
n <- dim(X_mod)[1]

iid_test_size <- 0.3
n_idd <- n - as.integer(n * iid_test_size)
set.seed(22)
iid_idx <- sample(1:n, n_idd, replace = F)
test_idx <- which(!(1:n %in% iid_idx))
k_fold <- 10
cv_idx <- which(1:n %in% iid_idx)
set.seed(44)
cv_folds <- caret::createFolds(cv_idx, k_fold)
```

Is has been chosen to use `r k_fold`-fold cross validation with a validation set on `r as.integer(iid_test_size*100)`% of the `r dim(dat[!is.na(dat$Y),])[1]` train observations.
By incresing the number of folds to 10 or to a more drametically approaches such as the leave-one-out cross validation it will usaually result in a lower bias but a greather variance in the estimate of the parameters. 

The three models are trained and validated on exactly the same indices. The MSE have been calculated for each loop in their hyperparameter search.

* The Ridge regression only depends on $\lambda_{ii}$ and is calculated on close form as follows: $\beta _{ \lambda _{ ii } }=(X_{ train }^{ T }X_{ train }+\lambda _{ ii }I)X_{ train }^{ T })^{ -1 }(Y_{train}-\mu _{ Y_{train} })$.
* The approach for calculating the Lasso regression is inspired by the: `glmnet(X_train, Y_train - Y_train_mean, lambda, alpha = 0, standardize = F, intercept=F)` function. The coefficients of the regression for a given $\lambda_{ii}$ can be extracted by the `coef()` function.
* The approach for calculating the Elastic net regression is inspired by the: `glmnet(X_train, Y_train - Y_train_mean, lambda, alpha = 0.5, standardize = F, intercept=F)` function. It has been chosen to force $\alpha=0.5$ which is the mid point between the Ridge regression and the Lasso regression.

Please notice that the mean of the responds are subtracted in both models. This is due to the characteristic of the shrinking methods. By not center the responds variable around its mean the model estimation will introduce a bias to the model.

The train design matrix will be standardized by the `scale()` function in `R` and the validate design matrix will be scaled according to the train design matrix in each fold.

```{r}
#
k <- 200
l_r <- c(-5,1)
lambdas <- pracma::logspace(l_r[1], l_r[2], k)
MSE_el <- MSE_las <- MSE <- array(rep(NA, k_fold*k*3), c(k_fold, k, 3)); 

for (kk in 1:k_fold) {
    #
    test_idx <- cv_folds[[kk]]
    train_idx <- which(!(cv_idx %in% test_idx))
    # preprocessing and subtracting the train and data set
    X_train <- X_mod %>% 
        slice(train_idx) %>% 
        as.matrix() %>% 
        scale(.)
    Y_train <- Y_mod %>% slice(train_idx) %>% as.matrix(.)
    Y_train_mean <- mean(Y_train)
    #
    X_val <- X_mod %>% 
        slice(test_idx) %>% 
        as.matrix(.) %>% 
        scale(., center = attr(X_train,"scaled:center"), scale = attr(X_train,"scaled:scale"))
    Y_val <- Y_mod %>% slice(test_idx) %>% as.matrix(.)
    
    # Ridge and Lasso
    for(ii in 1:k) {
        # estimate ridge parameters
        beta_r <- solve(t(X_train) %*% X_train + lambdas[ii] * diag(p)) %*% t(X_train) %*% (Y_train-Y_train_mean)
        # caculate error
        res <- (Y_val-(Y_train_mean + X_val %*% beta_r))^2
        MSE[kk, ii, 1] <- mean(res)
        # MSE[kk, ii, 2] <- sd(res)
        # MSE[kk, ii, 3] <- var(res)

        # estimate lasso parameters
        beta_las <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0, lambda = lambdas[ii], standardize = F, intercept=F)), ncol=1)[-1,]
        # caculate error
        res <- (Y_val-(Y_train_mean + X_val %*% beta_las))^2
        MSE_las[kk, ii, 1] <- mean(res)
        # MSE_las[kk, ii, 2] <- sd(res)
        # MSE_las[kk, ii, 3] <- var(res)
        
        # estimate lasso parameters
        beta_el <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0.5, lambda = lambdas[ii], standardize = F, intercept=F)), ncol=1)[-1,]
        # caculate error
        res <- (Y_val-(Y_train_mean + X_val %*% beta_el))^2
        MSE_el[kk, ii, 1] <- mean(res)
        # MSE_el[kk, ii, 2] <- sd(res)
        # MSE_el[kk, ii, 3] <- var(res)
        
    }
}
```

Figure \ref{fig_1} illustrates the MSE as a function of $\lambda$ for the three models. 

```{r, fig.cap="\\label{fig_1}MSE as a function of lambda for the Ridge regression and the Lasso regression."}
# finds the average value for MSE for all folds as function of lambda
cv_df <- data.frame(lambda = lambdas)
cv_df$mse_means <- colMeans(MSE[,,1])
cv_df$mse_sd <- cv_df$mse_means / sqrt(k_fold)
best_idx_mean <- which.min(cv_df$mse_means)
best_idx_sd <- which.min(cv_df$mse_sd)
# update index to std. one error
best_idx_sd <- max(which(cv_df$mse_means[best_idx_sd] + cv_df$mse_sd[best_idx_sd] > cv_df$mse_means))
lambda_opt <- cv_df$lambda[best_idx_sd]

# finds the average value for MSE for all folds as function of lambda
cv_df_las <- data.frame(lambda = lambdas)
cv_df_las$mse_means <- colMeans(MSE_las[,,1])
cv_df_las$mse_sd <- cv_df_las$mse_means / sqrt(k_fold)
best_idx_mean_las <- which.min(cv_df_las$mse_means)
best_idx_sd_las <- which.min(cv_df_las$mse_sd)
# update index to std. one error
best_idx_sd_las <- max(which(cv_df_las$mse_means[best_idx_sd_las] + cv_df_las$mse_sd[best_idx_sd_las] > cv_df_las$mse_means))
lambda_opt_las <- cv_df_las$lambda[best_idx_sd_las]


# finds the average value for MSE for all folds as function of lambda
cv_df_el <- data.frame(lambda = lambdas)
cv_df_el$mse_means <- colMeans(MSE_el[,,1])
cv_df_el$mse_sd <- cv_df_el$mse_means / sqrt(k_fold)
best_idx_mean_el <- which.min(cv_df_el$mse_means)
best_idx_sd_el <- which.min(cv_df_el$mse_sd)
# update index to std. one error
best_idx_sd_el <- max(which(cv_df_el$mse_means[best_idx_sd_el] + cv_df_el$mse_sd[best_idx_sd_el] > cv_df_el$mse_means))
lambda_opt_el <- cv_df_el$lambda[best_idx_sd_el]



# std. one error plot
ggplot() +
    geom_point(data=cv_df, aes(lambda, mse_means, colour="MSE Ridge")) +
    geom_point(data=cv_df_las, aes(lambda, mse_means, colour="MSE Lasso")) +
    geom_point(data=cv_df_el, aes(lambda, mse_means, colour="MSE Elastic net")) +
    geom_point(data=cv_df[best_idx_mean,], aes(lambda, mse_means, colour="opt. L_R")) +
    geom_point(data=cv_df_las[best_idx_mean_las,], aes(lambda, mse_means, colour="opt. L_L")) +
    geom_point(data=cv_df_el[best_idx_mean_el,], aes(lambda, mse_means, colour="opt. L_E")) +
    
    geom_point(data=cv_df[best_idx_sd,], aes(lambda, mse_means, colour=paste("one std. error\nL_R =",round(cv_df[best_idx_sd,"lambda"],3)))) +
    geom_point(data=cv_df_las[best_idx_sd_las,], aes(lambda, mse_means, colour=paste("one std. error\nL_L =",round(cv_df_las[best_idx_sd_las,"lambda"],3)))) +
    
    geom_point(data=cv_df_el[best_idx_sd_el,], aes(lambda, mse_means, colour=paste("one std. error\nL_E =",round(cv_df_el[best_idx_sd_el,"lambda"],3)))) +
    
    coord_trans(x="log") +
    scale_y_continuous(limits = c(0,2)) +
    scale_x_continuous(breaks = pracma::logspace(l_r[1], l_r[2], k/50)) +
    labs(x="log lambda (L)",y="MSE", colour="") +
    theme_TS()
```


## Model Assessment based upon `r as.integer(iid_test_size*100)`% test set
The estimated models are evaluated on the `r as.integer(iid_test_size*100)`% test set in order to ensure the best possible estimate of the `RMSE` of the unseen data. The models use the optimal parameters plus the one standard error rule which is found by the `r k_fold`-fold cross validation. The parameters are as follows:

* $\lambda_{Ridge} = `r lambda_opt`$
* $\lambda_{Lasso} = `r lambda_opt_las`$
* $\lambda_{Elastic\, net} = `r lambda_opt_el`$

Table \ref{tab_2} reports the performance metric of the `r as.integer(iid_test_size*100)`% validate set (model selection). 

```{r}
#
X_train <- X_mod %>% slice(test_idx) %>% as.matrix() %>% scale(.)
Y_train <- Y_mod %>% slice(test_idx) %>% as.matrix(.)
Y_train_mean <- mean(Y_train)
# Ridge
beta_r <- solve(t(X_train) %*% X_train + lambda_opt * diag(p)) %*% t(X_train) %*% (Y_train-Y_train_mean)
# get estimated resdisuals
Y_train_hat <- Y_train_mean + X_train %*% beta_r
r2 <- R2_func(Y_train, Y_train_hat)
rmse <- RMSE_func(Y_train, Y_train_hat)
mse <- MSE_func(Y_train, Y_train_hat)

# lasso
beta_las <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0, lambda = lambda_opt_las, standardize = F, intercept=F)), ncol=1)[-1,]
Y_train_hat_las <- Y_train_mean + X_train %*% beta_las
r2_las <- R2_func(Y_train, Y_train_hat_las)
rmse_las <- RMSE_func(Y_train, Y_train_hat_las)
mse_las <- MSE_func(Y_train, Y_train_hat_las)

# elastic net
beta_el <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0.5, lambda = lambda_opt_el, standardize = F, intercept=F)), ncol=1)[-1,]
Y_train_hat_el <- Y_train_mean + X_train %*% beta_el
r2_el <- R2_func(Y_train, Y_train_hat_el)
rmse_el <- RMSE_func(Y_train, Y_train_hat_el)
mse_el <- MSE_func(Y_train, Y_train_hat_el)

# do average
# r_num <- c(r2, r2_las, r2_rf)
r_num <- c(1-rmse, 1-rmse_las, 1-rmse_el)
r_num <- r_num / sum(r_num)

# how much of the variance is described?
Y_hat_comp <- Y_train_hat * r_num[1] + Y_train_hat_las * r_num[2] + Y_train_hat_el * r_num[3]
r2_comp <- R2_func(Y_train, Y_hat_comp)
rmse_comp <- RMSE_func(Y_train, Y_hat_comp)
mse_comp <- MSE_func(Y_train, Y_hat_comp)


knitr::kable(data.frame(Algo = c("Ridge Regression", "Lasso Regression", "Elastic net", "Ensemble Model"),
                        R2 = c(r2, r2_las, r2_el, r2_comp),
                        MSE = c(mse, mse_las, mse_el, mse_comp),
                        RMSE = c(rmse, rmse_las, rmse_el, rmse_comp)),
             caption = "\\label{tab_2}Performance of the three models and the combined ensemble model."
)
```

The bottom row of table \ref{tab_2} is the performance metrics for the ensemble model. The weight ratios between the three models are given below:

* $w_{Ridge}=\frac{(1-`r rmse`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse)/((1-rmse_las) + (1-rmse)+ (1-rmse_el))`$
* $w_{Lasso}=\frac{(1-`r rmse_las`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse_las)/((1-rmse_las) + (1-rmse)+ (1-rmse_el))`$
* $w_{Elastic}=\frac{(1-`r rmse_el`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse_el)/((1-rmse_las) + (1-rmse) + (1-rmse_el))`$

### Expected RMSE
The expected `RMSE` is: 

* `RMSE=``r rmse_comp` 

### Re-train the model on complete train data
The models will be re-trained on the complete train data after the unbiased `RMSE` estimate has been stated. New parameter estimates are obtained and new weight ratios used in the ensemble are calculated.

```{r}
#
X_train <- X_mod %>% as.matrix() %>% scale(.)
Y_train <- Y_mod  %>% as.matrix(.)
Y_train_mean <- mean(Y_train)
# Ridge
beta_r <- solve(t(X_train) %*% X_train + lambda_opt * diag(p)) %*% t(X_train) %*% (Y_train-Y_train_mean)
# get estimated resdisuals
Y_train_hat <- Y_train_mean + X_train %*% beta_r
r2 <- R2_func(Y_train, Y_train_hat)
rmse <- RMSE_func(Y_train, Y_train_hat)
mse <- MSE_func(Y_train, Y_train_hat)

# lasso
beta_las <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0, lambda = lambda_opt_las, standardize = F, intercept=F)), ncol=1)[-1,]
Y_train_hat_las <- Y_train_mean + X_train %*% beta_las
r2_las <- R2_func(Y_train, Y_train_hat_las)
rmse_las <- RMSE_func(Y_train, Y_train_hat_las)
mse_las <- MSE_func(Y_train, Y_train_hat_las)

# elastic net
beta_el <- as.matrix(coef(glmnet::glmnet(X_train, Y_train - Y_train_mean, alpha = 0.5, lambda = lambda_opt_el, standardize = F, intercept=F)), ncol=1)[-1,]
Y_train_hat_el <- Y_train_mean + X_train %*% beta_el
r2_el <- R2_func(Y_train, Y_train_hat_el)
rmse_el <- RMSE_func(Y_train, Y_train_hat_el)
mse_el <- MSE_func(Y_train, Y_train_hat_el)


# do average
# r_num <- c(r2, r2_las, r2_rf)
r_num <- c(1-rmse, 1-rmse_las, 1-rmse_el)
r_num <- r_num / sum(r_num)

# save model objects
save(X_tt, X_train, Y_train, Y_train_mean, r_num,rmse_comp, beta_r, beta_las, beta_el, file="~/DTU/Courses/CDA/Cases/case1/model_objects.rda")
```

The new weight ratios between the three models are slightly changed compare to weights found by using the validation set (model selection). The weights for the ensemble model are given below:

* $w_{Ridge}=\frac{(1-`r rmse`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse)/((1-rmse_las) + (1-rmse)+ (1-rmse_el))`$
* $w_{Lasso}=\frac{(1-`r rmse_las`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse_las)/((1-rmse_las) + (1-rmse)+ (1-rmse_el))`$
* $w_{Elastic}=\frac{(1-`r rmse_el`)}{(1-`r rmse`)+(1-`r rmse_las`)+(1-`r rmse_el`)}=`r (1-rmse_el)/((1-rmse_las) + (1-rmse) + (1-rmse_el))`$

<!--
## Reflections

What could be improved and why?

* ...
-->

## Achieved RMSE of the missing respondses 
COMING UP!

```{r}
rm(list=ls())
library(dplyr)
source("~/DTU/Courses/CDA/Cases/case1/funcs.R")
load("~/DTU/Courses/CDA/Cases/case1/model_objects.rda")

# scale data
X_unknown <- X_tt %>% scale(., center = attr(X_train,"scaled:center"), 
                            scale = attr(X_train,"scaled:scale")) %>% 
    as.matrix(.)

# rigde regression
Y_unknown_hat <- Y_train_mean + X_unknown %*% beta_r
# lasso
Y_unknown_hat_las <- Y_train_mean + X_unknown %*% beta_las
# elastic net
Y_unknown_hat_el <- Y_train_mean + X_unknown %*% beta_el

# estimate response

Y_unknown_hat_complete <- Y_unknown_hat * r_num[1] + Y_unknown_hat_las * r_num[2] + Y_unknown_hat_el * r_num[3]
write(Y_unknown_hat_complete, "~/DTU/Courses/CDA/Cases/case1/Y_hat.csv")
save(Y_unknown_hat_complete, file = "~/DTU/Courses/CDA/Cases/case1/Y_hat.Rda")



# load correct value of Y[101:1100]
Y_test_hat_correct <- readr::read_csv("~/DTU/Courses/CDA/Cases/case1/Case1_Answer.csv")[["Y"]][101:1100]
# source("~/DTU/Courses/CDA/Cases/case1/Case1CheckY.R")
# RMSE_tot <- Case1CheckY(Y_unknown_hat_complete)
RMSE_tot <- RMSE_func(Y_test_hat_correct, Y_unknown_hat_complete)

```

* Achieved RMSE of the 1000 observations: `RMSE=``r RMSE_tot`
